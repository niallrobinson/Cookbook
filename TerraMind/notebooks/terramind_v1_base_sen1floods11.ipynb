{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157ace80",
      "metadata": {
        "id": "157ace80"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 International Business Machines\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#  http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bacc318390456b",
      "metadata": {
        "id": "b4bacc318390456b"
      },
      "source": [
        "## Overview\n",
        "This notebook focuses on finetuning the [terramind model](https://huggingface.co/collections/ibm-esa-geospatial/terramind-10-68063b522bfbeda3498aeafe) to identify floods in a sentinel scene. The main takeaways from this notebook will be as follows:\n",
        "1. Learn how to use Terratorch to fine-tune terramind for floods in sentinel scene.\n",
        "2. Understand the effects of specific parameters in training and hardware utilization.\n",
        "3. Use fine-tuned model for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a842123",
      "metadata": {
        "id": "5a842123"
      },
      "source": [
        "## Setup\n",
        "1. Install terratorch - To install the necessary packages, execute the cell below. This will take a few minutes. Once the installation process is done, a window will pop up to ask you to restart the session. This is normal and you should proceed to restart using the interface in the pop up window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb818b6e",
      "metadata": {
        "id": "eb818b6e"
      },
      "outputs": [],
      "source": [
        "!pip install terratorch==1.0.2 gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0c3439",
      "metadata": {
        "id": "ad0c3439"
      },
      "source": [
        "2. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
      "metadata": {
        "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gdown\n",
        "import terratorch\n",
        "import albumentations\n",
        "import lightning.pytorch as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917b65b8e7cd7d65",
      "metadata": {
        "id": "917b65b8e7cd7d65"
      },
      "source": [
        "3. Download the dataset from Google Drive. This should take a few minutes. Sen1Floods11 is a surface water data set including classified permanent water, flood water, and raw Sentinel-1 imagery. The full dataset consists of 4,831 labeled 512x512 chips that span all 14 biomes, 357 ecoregions, and 6 continents of the world across 11 flood events. To learn more about the dataset used in this notebook, take a look at the Sen1Floods11 [github page](https://github.com/cloudtostreet/Sen1Floods11?tab=readme-ov-file) and associated [publication](https://openaccess.thecvf.com/content_CVPRW_2020/html/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dw5-9A4A4OmI",
      "metadata": {
        "id": "dw5-9A4A4OmI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!mkdir ../data/\n",
        "if not os.path.isfile(\"../data/sen1floods11_v1.1.tar.gz\"):\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1lRw3X7oFNq_WyzBO6uyUJijyTuYm23VS\", '../data/sen1floods11_v1.1.tar.gz')\n",
        "\n",
        "!tar -xzf ../data/sen1floods11_v1.1.tar.gz -C ../data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
      "metadata": {
        "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
      },
      "source": [
        "## Sen1Floods11 Dataset\n",
        "\n",
        "Lets start with analysing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
      "metadata": {
        "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path(\"../data/sen1floods11_v1.1\")\n",
        "!ls \"../data/sen1floods11_v1.1/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d91245594c607d",
      "metadata": {
        "id": "87d91245594c607d"
      },
      "outputs": [],
      "source": [
        "!ls \"../data/sen1floods11_v1.1/data/S2L1CHand/\" | head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f22dc984ead544",
      "metadata": {
        "id": "a2f22dc984ead544"
      },
      "source": [
        "TerraTorch provides generic data modules that work directly with PyTorch Lightning.\n",
        "\n",
        "Sen1Floods11 is a multimodal dataset that provides Sentinel-2 L2A and Sentinel-1 GRD data.\n",
        "Therefore, we are using the `GenericMultiModalDataModule`.\n",
        "This module is similar to the `GenericNonGeoSegmentationDataModule`, which is used for standard segmentation tasks.\n",
        "However, the data roots, `img_grep` are other settings are provided as dict to account for the multimodal inputs. You find all settings in the [documentation](https://ibm.github.io/terratorch/stable/generic_datamodules/).\n",
        "In a Lightning config, the data module is defined with the `data` key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
      "metadata": {
        "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
      },
      "outputs": [],
      "source": [
        "data_mean = {\n",
        "    \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
        "    \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
        "    \"S1GRD\": [-12.599, -20.293],\n",
        "    \"S1RTC\": [-10.93, -17.329],\n",
        "    \"RGB\": [87.271, 80.931, 66.667],\n",
        "    \"DEM\": [670.665]\n",
        "}\n",
        "\n",
        "data_stds = {\n",
        "    \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
        "    \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
        "    \"S1GRD\": [5.195, 5.890],\n",
        "    \"S1RTC\": [4.391, 4.459],\n",
        "    \"RGB\": [58.767, 47.663, 42.631],\n",
        "    \"DEM\": [951.272],\n",
        "}\n",
        "\n",
        "datamodule = terratorch.datamodules.GenericMultiModalDataModule(\n",
        "    task=\"segmentation\",\n",
        "    batch_size=4,\n",
        "    num_workers=2,\n",
        "    num_classes=2,\n",
        "\n",
        "    # Define your input modalities. The names must match the keys in the following dicts.\n",
        "    modalities=[\"S2L1C\", \"S1GRD\"],\n",
        "    rgb_modality=\"S2L1C\",  # Used for plotting. Defaults to the first modality if not provided.\n",
        "    rgb_indices=[3,2,1],  # RGB channel positions in the rgb_modality.\n",
        "\n",
        "    # Define data paths as dicts using the modality names as keys.\n",
        "    train_data_root={\n",
        "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
        "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
        "    },\n",
        "    train_label_data_root=dataset_path / \"data/LabelHand\",\n",
        "    val_data_root={\n",
        "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
        "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
        "    },\n",
        "    val_label_data_root=dataset_path / \"data/LabelHand\",\n",
        "    test_data_root={\n",
        "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
        "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
        "    },\n",
        "    test_label_data_root=dataset_path / \"data/LabelHand\",\n",
        "\n",
        "    # Define split files because all samples are saved in the same folder.\n",
        "    train_split=dataset_path / \"splits/flood_train_data.txt\",\n",
        "    val_split=dataset_path / \"splits/flood_valid_data.txt\",\n",
        "    test_split=dataset_path / \"splits/flood_test_data.txt\",\n",
        "\n",
        "    # Define suffix, again using dicts.\n",
        "    img_grep={\n",
        "        \"S2L1C\": \"*_S2Hand.tif\",\n",
        "        \"S1GRD\": \"*_S1Hand.tif\",\n",
        "    },\n",
        "    label_grep=\"*_LabelHand.tif\",\n",
        "\n",
        "    # With TerraTorch, you can select a subset of the dataset bands as model inputs by providing dataset_bands (all bands in the data) and output_bands (selected bands). This setting is optional for all modalities and needs to be provided as dicts.\n",
        "    # Here is an example for with S-1 GRD. You could change the output to [\"VV\"] to only train on the first band. Note that means and stds must be aligned with the output_bands (equal length of values).\n",
        "    dataset_bands={\n",
        "        \"S1GRD\": [\"VV\", \"VH\"]\n",
        "    },\n",
        "    output_bands={\n",
        "        \"S1GRD\": [\"VV\", \"VH\"]\n",
        "    },\n",
        "\n",
        "    # Define standardization values. We use the pre-training values here and providing the additional modalities is not a problem, which makes it simple to experiment with different modality combinations. Alternatively, use the dataset statistics that you can generate using `terratorch compute_statistics -c config.yaml` (requires concat_bands: true for this multimodal datamodule).\n",
        "    means=data_mean,\n",
        "    stds=data_stds,\n",
        "\n",
        "        # albumentations supports shared transformations and can handle multimodal inputs.\n",
        "    train_transform=[\n",
        "        albumentations.D4(), # Random flips and rotation\n",
        "        albumentations.pytorch.transforms.ToTensorV2(),\n",
        "    ],\n",
        "    val_transform=None,  # Using ToTensorV2() by default if not provided\n",
        "    test_transform=None,\n",
        "\n",
        "    no_label_replace=-1,  # Replace NaN labels. defaults to -1 which is ignored in the loss and metrics.\n",
        "    no_data_replace=0,  # Replace NaN data\n",
        ")\n",
        "\n",
        "# Setup train and val datasets\n",
        "datamodule.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
      "metadata": {
        "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
      },
      "outputs": [],
      "source": [
        "# checking datasets validation split size\n",
        "val_dataset = datamodule.val_dataset\n",
        "len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ea2e33e1739248",
      "metadata": {
        "id": "20ea2e33e1739248"
      },
      "outputs": [],
      "source": [
        "# checking datasets testing split size\n",
        "datamodule.setup(\"test\")\n",
        "test_dataset = datamodule.test_dataset\n",
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
      "metadata": {
        "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01"
      },
      "outputs": [],
      "source": [
        "# plotting a sample (The code only plots the defined `rgb_modality`)\n",
        "val_dataset.plot(val_dataset[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770f9c2593c90b79",
      "metadata": {
        "id": "770f9c2593c90b79"
      },
      "outputs": [],
      "source": [
        "# Helper function for plotting both modalities\n",
        "def plot_sample(sample):\n",
        "    s1 = sample['image']['S1GRD'].cpu().numpy()\n",
        "    s2 = sample['image']['S2L1C'].cpu().numpy()\n",
        "    mask = sample['mask'].cpu().numpy()\n",
        "\n",
        "    # Scaling data. Using -30 to 0 scaling for S-1 and 0 - 2000 for S-2. S-1 is visualized as [VH, VV, VH]\n",
        "    s1 = (s1.clip(-30, 0) / 30 + 1) * 255\n",
        "    s2 = (s2.clip(0, 2000) / 2000) * 255\n",
        "    s1_rgb = np.stack([s1[1], s1[0], s1[1]], axis=0).astype(np.uint8).transpose(1,2,0)\n",
        "    s2_rgb = s2[[3,2,1]].astype(np.uint8).transpose(1,2,0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    ax[0].imshow(s1_rgb)\n",
        "    ax[0].set_title('S-1 GRD')\n",
        "    ax[0].axis('off')\n",
        "    ax[1].imshow(s2_rgb)\n",
        "    ax[1].set_title('S-2 GRD')\n",
        "    ax[1].axis('off')\n",
        "    ax[2].imshow(mask, vmin=-1, vmax=1, interpolation='nearest')\n",
        "    ax[2].set_title('Mask')\n",
        "    ax[2].axis('off')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb92b1d0599a2c2",
      "metadata": {
        "id": "eeb92b1d0599a2c2"
      },
      "outputs": [],
      "source": [
        "plot_sample(val_dataset[2])\n",
        "plot_sample(val_dataset[8])\n",
        "plot_sample(val_dataset[11])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0453502fb0bf62",
      "metadata": {
        "id": "cf0453502fb0bf62"
      },
      "source": [
        "# TerraTorch model registry\n",
        "\n",
        "TerraTorch includes its own backbone registry with many EO FMs. It also includes meta registries for all model components that include other sources like timm image models or SMP decoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d970183baaea88cd",
      "metadata": {
        "id": "d970183baaea88cd"
      },
      "outputs": [],
      "source": [
        "from terratorch.registry import BACKBONE_REGISTRY, TERRATORCH_BACKBONE_REGISTRY, TERRATORCH_DECODER_REGISTRY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4109f8f262cc5f6",
      "metadata": {
        "id": "f4109f8f262cc5f6"
      },
      "outputs": [],
      "source": [
        "# Print all TerraMind v1 backbones.\n",
        "[backbone\n",
        " for backbone in TERRATORCH_BACKBONE_REGISTRY\n",
        " if 'terramind_v1' in backbone]\n",
        "# TiM models are using the Thinking-in-Modalities approach, see our paper for details https://arxiv.org/html/2504.11171v1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a51fdde4e1e5d1a",
      "metadata": {
        "id": "9a51fdde4e1e5d1a"
      },
      "outputs": [],
      "source": [
        "# Available decoders. We use the UNetDecoder in this example.\n",
        "list(TERRATORCH_DECODER_REGISTRY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bc7fa971e02793",
      "metadata": {
        "id": "56bc7fa971e02793"
      },
      "outputs": [],
      "source": [
        "# Use the backbone registry to load a PyTorch model for custom pipeline. The pre-trained weights are automatically downloaded with pretrained=True.\n",
        "model = BACKBONE_REGISTRY.build(\n",
        "    \"terramind_v1_base\",\n",
        "    modalities=[\"S2L1C\", \"S1GRD\"],\n",
        "    pretrained=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcb50e133f20cd7",
      "metadata": {
        "id": "9fcb50e133f20cd7"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654a30ddef8ed5a",
      "metadata": {
        "id": "654a30ddef8ed5a"
      },
      "source": [
        "# Fine-tune TerraMind via PyTorch Lightning\n",
        "\n",
        "With TerraTorch, we can use standard Lightning components for the fine-tuning.\n",
        "These include callbacks and the trainer class.\n",
        "\n",
        "TerraTorch provides EO-specific tasks that define the training and validation steps.\n",
        "In this case, we are using the `SemanticSegmentationTask` and using a unet decoder.\n",
        "\n",
        "We refer to the [TerraTorch paper](https://arxiv.org/abs/2503.20563) for a detailed explanation of the TerraTorch tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
      "metadata": {
        "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(0)\n",
        "\n",
        "output_folder = \"../output\"\n",
        "\n",
        "# By default, TerraTorch saves the model with the best validation loss. You can overwrite this by defining a custom ModelCheckpoint, e.g., saving the model with the highest validation mIoU.\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=f\"{output_folder}/terramind_base_sen1floods11/checkpoints/\",\n",
        "    mode=\"max\",\n",
        "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
        "    filename=\"best-mIoU\",\n",
        "    save_weights_only=True,\n",
        ")\n",
        "\n",
        "# Lightning Trainer\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    strategy=\"auto\",\n",
        "    devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
        "    precision=\"16-mixed\",  # Speed up training with half precision, delete for full precision training.\n",
        "    num_nodes=1,\n",
        "    logger=True,  # Uses TensorBoard by default\n",
        "    max_epochs=5, # For demos\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
        "    default_root_dir=f\"{output_folder}/terramind_base_sen1floods11/\",\n",
        ")\n",
        "\n",
        "# Segmentation mask that build the model and handles training and validation steps.\n",
        "model = terratorch.tasks.SemanticSegmentationTask(\n",
        "    model_factory=\"EncoderDecoderFactory\",  # Combines a backbone with necks, the decoder, and a head\n",
        "    model_args={\n",
        "        # TerraMind backbone\n",
        "        \"backbone\": 'terramind_v1_base', # large version: terramind_v1_large\n",
        "        \"backbone_pretrained\": True,\n",
        "        \"backbone_modalities\": [\"S2L1C\", \"S1GRD\"],\n",
        "        # Optionally, define the input bands. This is only needed if you select a subset of the pre-training bands, as explained above.\n",
        "        # \"backbone_bands\": {\"S1GRD\": [\"VV\"]},\n",
        "\n",
        "        # Necks\n",
        "        \"necks\": [\n",
        "            {\n",
        "                \"name\": \"SelectIndices\",\n",
        "                \"indices\": [2, 5, 8, 11] # indices for terramind_v1_base\n",
        "                # \"indices\": [5, 11, 17, 23] # indices for terramind_v1_large\n",
        "            },\n",
        "            {\"name\": \"ReshapeTokensToImage\",\n",
        "             \"remove_cls_token\": False},  # TerraMind is trained without CLS token, which neads to be specified.\n",
        "            {\"name\": \"LearnedInterpolateToPyramidal\"}  # Some decoders like UNet or UperNet expect hierarchical features. Therefore, we need to learn a upsampling for the intermediate embedding layers when using a ViT like TerraMind.\n",
        "        ],\n",
        "\n",
        "        # Decoder\n",
        "        \"decoder\": \"UNetDecoder\",\n",
        "        \"decoder_channels\": [512, 256, 128, 64],\n",
        "\n",
        "        # Head\n",
        "        \"head_dropout\": 0.1,\n",
        "        \"num_classes\": 2,\n",
        "    },\n",
        "\n",
        "    loss='dice',\n",
        "    optimizer=\"AdamW\",\n",
        "    lr=2e-5,\n",
        "    ignore_index=-1,\n",
        "    freeze_backbone=True, # Only used to speed up fine-tuning in this demo, we highly recommend fine-tuning the backbone for the best performance.\n",
        "    freeze_decoder=False,  # Should be false in most cases as the decoder is randomly initialized.\n",
        "    plot_on_val=True,  # Plot predictions during validation steps\n",
        "    class_names=[\"Others\", \"Water\"]  # optionally define class names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff284062edfce308",
      "metadata": {
        "id": "ff284062edfce308"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "trainer.fit(model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c1bebdb7370a174",
      "metadata": {
        "id": "3c1bebdb7370a174"
      },
      "source": [
        "# Test the fine-tuned model\n",
        "\n",
        "Let's gather and specify the relevant files for carrying out testing. Look for your .ckpt file produced during the fine-tuning process here it is in '../output/terramind_base_sen1floods11/checkpoints/best-mIoU.ckpt'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a77263-5308-4781-a17f-a35e62ca1875",
      "metadata": {
        "id": "35a77263-5308-4781-a17f-a35e62ca1875",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Let's test the fine-tuned model\n",
        "best_ckpt_path = f\"{output_folder}/terramind_base_sen1floods11/checkpoints/best-mIoU.ckpt\"\n",
        "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
        "\n",
        "# Note: This demo only trains for 5 epochs by default, which does not result in good test metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
      "metadata": {
        "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536"
      },
      "outputs": [],
      "source": [
        "# Now we can use the model for predictions and plotting\n",
        "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
        "    best_ckpt_path,\n",
        "    model_factory=model.hparams.model_factory,\n",
        "    model_args=model.hparams.model_args,\n",
        ")\n",
        "\n",
        "test_loader = datamodule.test_dataloader()\n",
        "with torch.no_grad():\n",
        "    batch = next(iter(test_loader))\n",
        "    images = batch[\"image\"]\n",
        "    for mod, value in images.items():\n",
        "        images[mod] = value.to(model.device)\n",
        "    masks = batch[\"mask\"].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "\n",
        "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
        "\n",
        "for i in range(4):\n",
        "    sample = {\n",
        "        \"image\": batch[\"image\"][\"S2L1C\"][i].cpu(),\n",
        "        \"mask\": batch[\"mask\"][i],\n",
        "        \"prediction\": preds[i],\n",
        "    }\n",
        "    test_dataset.plot(sample)\n",
        "    plt.show()\n",
        "\n",
        "# Note: This demo only trains for 5 epochs by default, which does not result in good predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e7cf70f5e06c3e",
      "metadata": {
        "id": "80e7cf70f5e06c3e"
      },
      "source": [
        "# Fine-tuning via CLI\n",
        "\n",
        "We also run the fine-tuning via a [CLI](https://ibm.github.io/terratorch/stable/quick_start/#training-with-lightning-tasks). All parameters we have specified in the notebook can be put in a [yaml]( ../configs/terramind_v1_base_sen1floods11.yaml), and can be run using the command below. Take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/tutorials/the_yaml_config/) for how to setup the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b578dfe",
      "metadata": {
        "id": "0b578dfe"
      },
      "outputs": [],
      "source": [
        "# First let's get the config files from github.com.\n",
        "!git init\n",
        "!git remote add origin https://github.com/IBM/ML4EO-workshop-2025.git\n",
        "!git fetch --all\n",
        "!git checkout origin/main -- \"TerraMind/configs/terramind_v1_base_sen1floods11.yaml\"\n",
        "!git checkout origin/main -- \"TerraMind/configs/terramind_v1_base_tim_lulc_sen1floods11.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1c41843f46666f",
      "metadata": {
        "id": "fd1c41843f46666f"
      },
      "outputs": [],
      "source": [
        "# Run fine-tuning\n",
        "!terratorch fit -c \"TerraMind/configs/terramind_v1_base_sen1floods11.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca494b36db1bfb37",
      "metadata": {
        "id": "ca494b36db1bfb37"
      },
      "outputs": [],
      "source": [
        "# Run Thinking-in-Modalities (TiM) with generated LULC tokens\n",
        "!terratorch fit -c \"TerraMind/configs/terramind_v1_base_tim_lulc_sen1floods11.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b094bf7",
      "metadata": {
        "id": "3b094bf7"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}